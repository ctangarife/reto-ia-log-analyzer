# Detector de Anomal√≠as en Logs

Sistema de detecci√≥n de anomal√≠as en logs utilizando Isolation Forest y LLM (Large Language Model) para an√°lisis y explicaci√≥n en lenguaje natural.

## √çndice
1. [Descripci√≥n General](#descripci√≥n-general)
2. [Arquitectura](#arquitectura)
3. [Instalaci√≥n Paso a Paso](#instalaci√≥n-paso-a-paso)
4. [Configuraci√≥n](#configuraci√≥n)
5. [Uso del Sistema](#uso-del-sistema)
6. [Modelos LLM Soportados](#modelos-llm-soportados)
7. [Estructura de Directorios](#estructura-de-directorios)
8. [Flujos Principales](#flujos-principales)
9. [Caracter√≠sticas Clave](#caracter√≠sticas-clave)
10. [Limitaciones y Consideraciones](#limitaciones-y-consideraciones)
11. [Soluci√≥n de Problemas (FAQ)](#soluci√≥n-de-problemas-faq)

## Descripci√≥n General

El detector de anomal√≠as en logs es un sistema completo que combina algoritmos de machine learning (Isolation Forest) con modelos de lenguaje (LLM) para:

1. Procesar archivos de logs de gran tama√±o
2. Detectar patrones an√≥malos o sospechosos
3. Proporcionar explicaciones en lenguaje natural sobre las anomal√≠as detectadas
4. Visualizar resultados de manera intuitiva

### üèÜ Caracter√≠sticas Destacadas

- ‚úÖ **Procesamiento de archivos grandes** (GB de logs)
- ‚úÖ **Modelo de IA configurable** (cualquier modelo de Ollama)
- ‚úÖ **Interfaz web intuitiva** con drag & drop
- ‚úÖ **An√°lisis en tiempo real** con streaming de resultados
- ‚úÖ **Explicaciones en lenguaje natural** de las anomal√≠as
- ‚úÖ **Historial persistente** de an√°lisis
- ‚úÖ **Escalable** con Docker y microservicios
- ‚úÖ **Soporte GPU/CPU** para mejor rendimiento

### üìä Stack Tecnol√≥gico

| Componente | Tecnolog√≠a | Versi√≥n | Prop√≥sito |
|-----------|------------|--------|----------|
| Frontend | Vue 3 + Vite | 3.3.0 | Interfaz de usuario |
| UI Library | PrimeVue | 3.40.0 | Componentes UI |
| Backend | FastAPI + Uvicorn | 0.104.1 | API REST |
| ML Engine | Scikit-learn | 1.3.2 | Isolation Forest |
| LLM Service | Ollama | 0.5.8 | Modelos de lenguaje |
| Databases | MongoDB + PostgreSQL + Redis | 7.0 / 15 / 7.2 | Almacenamiento |
| Proxy | Nginx | stable-alpine | Reverse proxy |
| Containerization | Docker + Compose | - | Orquestaci√≥n |

## Arquitectura

El sistema est√° compuesto por los siguientes servicios en contenedores Docker:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Vue UI     ‚îÇ     ‚îÇ  FastAPI Server ‚îÇ     ‚îÇ  Ollama Service ‚îÇ
‚îÇ   (Frontend)    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ(Anomaly Detect) ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    (LLM)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚ñ≤                       ‚îÇ                        ‚îÇ
        ‚îÇ                       ‚ñº                        ‚îÇ
        ‚îÇ               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ    Nginx        ‚îÇ              ‚ñº
                      ‚îÇ  (Proxy Server)  ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ  Base de Datos   ‚îÇ
                                             ‚îÇ  (MongoDB, etc.) ‚îÇ
                                             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Componentes

1. **Frontend (Vue3)**
   - Interfaz web para subida de archivos y visualizaci√≥n de resultados
   - Manejo de archivos grandes mediante chunking
   - Visualizaci√≥n en tiempo real del procesamiento
   - Historial de an√°lisis persistente

2. **Backend (FastAPI)**
   - API REST para procesamiento de logs
   - Detecci√≥n de anomal√≠as usando Isolation Forest
   - Integraci√≥n con Ollama para explicaciones en lenguaje natural
   - Procesamiento por chunks y streaming de resultados

3. **LLM (Ollama)**
   - Servicio local de LLM (modelo configurable)
   - Por defecto usa Qwen 2.5 3B, pero se puede usar cualquier modelo compatible con Ollama
   - Generaci√≥n de explicaciones en lenguaje natural
   - Procesamiento por lotes para optimizaci√≥n

4. **Nginx**
   - Proxy inverso
   - Manejo de archivos grandes
   - Configuraci√≥n para streaming

5. **Bases de Datos**
   - MongoDB: Almacenamiento de logs y reportes
   - PostgreSQL: Gesti√≥n de usuarios y configuraciones
   - Redis: Cach√© y gesti√≥n de colas de procesamiento

## Estructura de Directorios

```
reto-ia-log-analyzer/
‚îú‚îÄ‚îÄ build/                      # Dockerfiles y configuraciones
‚îÇ   ‚îú‚îÄ‚îÄ anomaly-detector/      # Servicio de detecci√≥n
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile         # Imagen del backend
‚îÇ   ‚îú‚îÄ‚îÄ ollama/               # Servicio LLM
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile         # Imagen de Ollama
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ init-ollama.sh     # Script de inicializaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ ui/                   # Frontend
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile         # Imagen del frontend
‚îÇ   ‚îú‚îÄ‚îÄ mongodb/              # Configuraci√≥n MongoDB
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ init-mongo.js      # Script de inicializaci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ redis/                # Configuraci√≥n Redis
‚îÇ       ‚îî‚îÄ‚îÄ redis.conf         # Archivo de configuraci√≥n
‚îú‚îÄ‚îÄ data/                       # C√≥digo de la aplicaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ anomaly-detector/      # Backend (FastAPI)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py            # API principal
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt    # Dependencias Python
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config/            # Configuraciones
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database/          # Scripts de BD
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ init.sql       # Inicializaci√≥n PostgreSQL
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scripts/           # Scripts auxiliares
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chunks/            # Almacenamiento temporal
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ reports/           # Reportes generados
‚îÇ   ‚îú‚îÄ‚îÄ ui/                    # Frontend (Vue3)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ package.json       # Dependencias Node.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/    # Componentes Vue
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stores/        # Estado global (Pinia)
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils/         # Utilidades
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dist/              # Archivos compilados
‚îÇ   ‚îú‚îÄ‚îÄ models/                # Modelos LLM descargados
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ollama/            # Modelos de Ollama
‚îÇ   ‚îî‚îÄ‚îÄ static/                # Archivos est√°ticos servidos por Nginx
‚îú‚îÄ‚îÄ nginx/                      # Configuraci√≥n del proxy
‚îÇ   ‚îî‚îÄ‚îÄ nginx.conf             # Configuraci√≥n Nginx
‚îî‚îÄ‚îÄ docker-compose.yml          # Orquestaci√≥n de servicios
```

## Flujos Principales

### 1. Procesamiento de Logs

1. **Subida y Chunking**
   ```javascript
   // Frontend: Divide archivo en chunks manejables
   const chunks = await splitLogFile(file)  // 500KB por chunk
   for (const chunk of chunks) {
     const formData = new FormData()
     formData.append('file', createChunkFile(chunk))
     // Env√≠o y procesamiento streaming...
   }
   ```

2. **Detecci√≥n de Anomal√≠as**
   ```python
   # Backend: Procesa cada chunk
   def detect_anomalies(log_lines):
       features = extract_features(log_lines)
       scores = isolation_forest.predict(features)
       return process_anomalies(log_lines, scores)
   ```

3. **Explicaci√≥n LLM**
   ```python
   # Backend: Procesa anomal√≠as en lotes
   async def process_anomalies_batch(anomalies):
       tasks = [get_llm_explanation(a) for a in anomalies]
       explanations = await asyncio.gather(*tasks)
       return combine_results(anomalies, explanations)
   ```

### 2. Gesti√≥n de Estado

1. **Store Global (Pinia)**
   ```typescript
   // Frontend: Manejo de estado
   const analysisStore = defineStore('analysis', {
     state: () => ({
       analysisHistory: [],
       currentAnalysis: null
     }),
     actions: {
       addAnalysis(result) {
         // Actualizaci√≥n de hist√≥rico...
       }
     }
   })
   ```

2. **Persistencia de Resultados**
   ```python
   # Backend: Guarda resultados por archivo
   def save_report(file_id, results):
       report_path = f"/app/chunks/{file_id}/report_{timestamp}.json"
       with open(report_path, 'w') as f:
           json.dump(results, f)
   ```

## Caracter√≠sticas Clave

1. **Procesamiento de Archivos Grandes**
   - Divisi√≥n en chunks de 500KB
   - Procesamiento incremental
   - Streaming de resultados
   - Progreso en tiempo real

2. **Detecci√≥n de Anomal√≠as**
   - Uso de Isolation Forest
   - Features: longitud, entrop√≠a, palabras clave
   - Scoring y clasificaci√≥n
   - Procesamiento paralelo

3. **Explicaciones IA**
   - Modelo local Nidum-Gemma-2B
   - Procesamiento por lotes
   - Prompts optimizados
   - Respuestas estructuradas

4. **UI/UX**
   - Carga de archivos con drag & drop
   - Visualizaci√≥n en tiempo real
   - Historial persistente
   - Agrupaci√≥n por archivos

## Ejemplos de Uso y Resultados

### Ejemplo de Log Normal
```
2024-01-15 10:30:15 INFO [UserService] User login successful: user123@example.com
2024-01-15 10:30:16 INFO [OrderService] Order created: ID=12345, User=user123
2024-01-15 10:30:17 INFO [PaymentService] Payment processed: $25.99
```
**Resultado**: Sin anomal√≠as detectadas

### Ejemplo de Log An√≥malo
```
2024-01-15 10:30:15 ERROR [AuthService] Multiple failed login attempts from 192.168.1.100
2024-01-15 10:30:16 ERROR [AuthService] SQL injection attempt detected: admin' OR '1'='1
2024-01-15 10:30:17 CRITICAL [SecurityService] Unauthorized access attempt to /admin/users
```
**Resultado**: ‚ö†Ô∏è **Anomal√≠a detectada** - Score: -0.85  
**Explicaci√≥n IA**: "Se detecta un posible ataque de fuerza bruta seguido de intento de inyecci√≥n SQL y acceso no autorizado. Recomiendo bloquear la IP 192.168.1.100 y revisar los logs de seguridad."

### Demo en Vivo

Puede probar el sistema con archivos de ejemplo:
1. `logs/ejemplo_normal.log` - Logs t√≠picos de aplicaci√≥n
2. `logs/ejemplo_anomalias.log` - Logs con patrones sospechosos
3. `logs/ejemplo_mixto.log` - Combinaci√≥n de logs normales y an√≥malos

## Instalaci√≥n Paso a Paso

> üöÄ **¬øTienes prisa?** Ve a la [Gu√≠a de Instalaci√≥n R√°pida](./INSTALACION-RAPIDA.md) para tenerlo funcionando en 5 minutos.

### 1. Requisitos Previos

- **Sistema Operativo**: Windows, macOS o Linux
- **Docker**: v20.10 o superior
- **Docker Compose**: v2.0 o superior
- **Hardware Recomendado**:
  - CPU: 4 cores o m√°s
  - RAM: 16GB m√≠nimo (32GB recomendado para mejores resultados)
  - Almacenamiento: 10GB libres m√≠nimo
  - GPU: NVIDIA compatible con CUDA (opcional, mejora significativamente el rendimiento del LLM)

### 2. Instalaci√≥n B√°sica

```bash
# Clonar el repositorio
git clone https://github.com/ctangarife/reto-ia-log-analyzer.git
cd reto-ia-log-analyzer

# Levantar los servicios
docker-compose up -d
```

### 3. Verificaci√≥n de Instalaci√≥n

Una vez iniciados los servicios, puede verificar que todo est√© funcionando correctamente:

```bash
# Ver estado de los contenedores
docker-compose ps

# Ver logs de los servicios
docker-compose logs -f
```

Acceda a la interfaz web a trav√©s de http://localhost:80

## Configuraci√≥n

### Configuraci√≥n del LLM (Modelo de Lenguaje)

**El modelo de lenguaje es totalmente configurable y opcional**. Por defecto, el sistema usa `qwen2.5:3b`, pero puede configurar cualquier modelo compatible con Ollama.

Para cambiar el modelo, edite las siguientes l√≠neas en el archivo `docker-compose.yml`:

```yaml
# En el servicio anomaly-detector
environment:
  - OLLAMA_SERVICE_URL=http://ollama-service:11434
  - MODEL_NAME=qwen2.5:3b  # Cambie a su modelo preferido

# En el servicio ollama-service
environment:
  - OLLAMA_HOST=0.0.0.0
  - OLLAMA_DEVICE=nvidia
  - OLLAMA_MODEL=qwen2.5:3b  # Cambie a su modelo preferido
```

Modelos recomendados:
- `llama3:8b` - Mayor calidad pero requiere m√°s recursos
- `gemma:7b` - Buen balance calidad/rendimiento
- `phi3:mini` - Modelo ligero para equipos con recursos limitados
- Cualquier modelo compatible con Ollama

### Configuraci√≥n del Sistema

#### Ajustes de Hardware

Ajuste los l√≠mites de recursos en `docker-compose.yml` seg√∫n las capacidades de su sistema:

```yaml
# Para el servicio de Ollama (LLM)
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: all
          capabilities: [ gpu ]
    limits:
      memory: 16G  # Ajuste seg√∫n la RAM disponible
```

#### Par√°metros de Detecci√≥n

Para ajustar la sensibilidad del detector de anomal√≠as, edite el archivo `data/anomaly-detector/main.py`:

```python
isolation_forest = IsolationForest(
    contamination=0.1,  # Porcentaje de anomal√≠as esperado (0.1 = 10%)
    random_state=42,
    n_estimators=100  # M√°s estimadores = mayor precisi√≥n pero m√°s lento
)
```

## Uso del Sistema

### 1. Acceso a la Interfaz

Abra su navegador y vaya a http://localhost:80

### 2. An√°lisis de Logs

1. **Subir Archivo**: Arrastre y suelte su archivo de logs o haga clic en el √°rea de carga
2. **Configuraci√≥n de An√°lisis**: Establezca los par√°metros seg√∫n necesite
3. **Iniciar An√°lisis**: Haga clic en el bot√≥n "Analizar"
4. **Ver Resultados**: Los resultados se mostrar√°n en tiempo real a medida que se procesan

### 3. Gesti√≥n de Reportes

- Los reportes se guardan autom√°ticamente y est√°n disponibles en la secci√≥n "Historial"
- Cada reporte incluye estad√≠sticas generales y detalles de las anomal√≠as detectadas
- Puede exportar los resultados en formato JSON o CSV

## Modelos LLM Soportados

El sistema es compatible con cualquier modelo disponible en Ollama. La elecci√≥n del modelo depender√° del hardware disponible y la calidad deseada de las explicaciones.

### ¬øC√≥mo cambiar el modelo?

1. **Opci√≥n 1**: Cambiar la variable de entorno en docker-compose.yml (como se explic√≥ anteriormente)

2. **Opci√≥n 2**: Usar un modelo ya descargado
   ```bash
   # Primero descargar el modelo deseado
   docker exec logs-analyze-ollama ollama pull llama3:8b
   
   # Luego editar docker-compose.yml y reiniciar los servicios
   docker-compose down
   docker-compose up -d
   ```

3. **Opci√≥n 3**: Crear un modelo personalizado
   ```bash
   # Conectarse al contenedor de Ollama
   docker exec -it logs-analyze-ollama bash
   
   # Crear un modelo personalizado
   cat > /tmp/Modelfile << EOF
   FROM llama3:8b
   PARAMETER temperature 0.7
   PARAMETER stop "User:"
   PARAMETER stop "Assistant:"
   EOF
   
   # Registrar el modelo
   ollama create mi-modelo-personalizado -f /tmp/Modelfile
   
   # Salir del contenedor
   exit
   ```
   Luego actualice las variables de entorno en docker-compose.yml con `MODEL_NAME=mi-modelo-personalizado`

### Configuraci√≥n Avanzada de Modelos

Para configuraciones m√°s detalladas, ejemplos espec√≠ficos por hardware y scripts de automatizaci√≥n, consulte el documento [CONFIGURACION-MODELOS.md](./CONFIGURACION-MODELOS.md).

## Limitaciones y Consideraciones

1. **Rendimiento**
   - Tama√±o de chunk afecta memoria y velocidad
   - LLM puede ser cuello de botella (especialmente sin GPU)
   - Considerar batch size vs latencia
   - Tiempo de descarga inicial del modelo puede ser significativo

2. **Almacenamiento**
   - Chunks y reportes ocupan espacio
   - Modelos LLM pueden requerir varios GB de almacenamiento
   - Implementar limpieza peri√≥dica
   - Monitorear uso de disco

3. **Escalabilidad**
   - Vertical: Aumentar recursos (especialmente RAM para modelos grandes)
   - Horizontal: M√∫ltiples workers
   - Cach√© de LLM para respuestas comunes
   - Considerar modelos m√°s peque√±os para mayor eficiencia

## Soluci√≥n de Problemas (FAQ)

### Problemas Comunes de Instalaci√≥n

#### Error: "No se puede conectar a Docker"
**Soluci√≥n**:
1. Verificar que Docker Desktop est√© ejecut√°ndose
2. Verificar permisos: `docker run hello-world`
3. Reiniciar Docker Desktop si es necesario

#### Error: "Out of memory" durante la descarga del modelo
**Soluci√≥n**:
```yaml
# En docker-compose.yml, reducir l√≠mites de memoria:
deploy:
  resources:
    limits:
      memory: 8G  # Reducir de 16G a 8G
```
O usar un modelo m√°s peque√±o como `phi3:mini`

#### El servicio Ollama no responde
**Soluci√≥n**:
```bash
# Verificar estado del contenedor
docker logs logs-analyze-ollama

# Si necesita reiniciar solo el servicio Ollama
docker-compose restart ollama-service
```

### Problemas de Rendimiento

#### El an√°lisis de logs es muy lento
**Soluciones**:
1. **Sin GPU**: Usar modelos m√°s peque√±os
   ```yaml
   environment:
     - MODEL_NAME=phi3:mini  # Modelo ligero
   ```

2. **Con GPU**: Verificar que NVIDIA Docker est√© instalado
   ```bash
   # Verificar GPU disponible
   docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi
   ```

3. **Ajustar tama√±o de chunk** en el backend

#### La descarga del modelo toma demasiado tiempo
**Opciones**:
1. Usar un modelo predesacargado
2. Cambiar a un modelo m√°s peque√±o temporalmente
3. Verificar conexi√≥n a internet

### Problemas de Configuraci√≥n

#### ¬øC√≥mo usar el sistema sin GPU?
**Configuraci√≥n**:
```yaml
# Comentar o eliminar la secci√≥n de GPU en docker-compose.yml
# deploy:
#   resources:
#     reservations:
#       devices:
#         - driver: nvidia
#           count: all
#           capabilities: [ gpu ]

# Cambiar variables de entorno
environment:
  - OLLAMA_DEVICE=cpu
  - MODEL_NAME=phi3:mini  # Usar modelo ligero
```

#### ¬øC√≥mo cambiar la base de datos?
Por defecto usa MongoDB, PostgreSQL y Redis. Para usar solo una:
```yaml
# En docker-compose.yml, comentar los servicios no deseados
# y ajustar las variables de entorno en anomaly-detector
```

#### ¬øC√≥mo ajustar la sensibilidad de detecci√≥n?
Edite `data/anomaly-detector/main.py`:
```python
# Valores m√°s bajos = m√°s sensible (m√°s anomal√≠as detectadas)
isolation_forest = IsolationForest(
    contamination=0.05,  # Cambiar de 0.1 a 0.05 para m√°s sensibilidad
    random_state=42,
    n_estimators=100
)
```

### Comandos √ötiles

#### Monitoreo del Sistema
```bash
# Ver uso de recursos
docker stats

# Ver logs en tiempo real
docker-compose logs -f

# Ver logs de un servicio espec√≠fico
docker-compose logs -f anomaly-detector

# Ver modelos descargados
docker exec logs-analyze-ollama ollama list
```

#### Limpieza y Mantenimiento
```bash
# Limpiar archivos temporales
docker-compose down
docker system prune -f

# Limpiar vol√∫menes (¬°CUIDADO: Elimina todos los datos!)
docker-compose down -v

# Reconstruir im√°genes
docker-compose build --no-cache
```

#### Backup y Restauraci√≥n
```bash
# Backup de los datos
docker run --rm -v logsanomaly_mongodb_data:/data -v $(pwd):/backup ubuntu tar czf /backup/mongodb-backup.tar.gz -C /data .

# Backup de los modelos
docker run --rm -v logs-analyze-ollama_models:/models -v $(pwd):/backup ubuntu tar czf /backup/models-backup.tar.gz -C /models .
```

### Contacto y Soporte

- **Repositorio**: [GitHub](https://github.com/ctangarife/reto-ia-log-analyzer)
- **Issues**: Para reportar bugs o solicitar caracter√≠sticas en [GitHub Issues](https://github.com/ctangarife/reto-ia-log-analyzer/issues)
- **Documentaci√≥n**: Wiki del repositorio para informaci√≥n adicional

### Contribuir al Proyecto

1. Fork del repositorio
2. Crear rama para nueva caracter√≠stica: `git checkout -b feature/nueva-caracteristica`
3. Commit de cambios: `git commit -am 'Add nueva-caracteristica'`
4. Push a la rama: `git push origin feature/nueva-caracteristica`
5. Crear Pull Request

## Licencia

Este proyecto est√° licenciado bajo la Licencia MIT. Consulte el archivo [LICENSE](LICENSE) para m√°s detalles.

## Roadmap y Caracter√≠sticas Futuras

### üó∫Ô∏è Pr√≥ximas Caracter√≠sticas

- [ ] **API de integraci√≥n**: Endpoints para sistemas externos
- [ ] **Alertas en tiempo real**: Notificaciones por email/Slack
- [ ] **Dashboard avanzado**: M√©tricas y gr√°ficos detallados
- [ ] **Modelos personalizados**: Entrenamiento con datos espec√≠ficos
- [ ] **Soporte multi-tenant**: Separaci√≥n por organizaciones
- [ ] **Exportaci√≥n avanzada**: PDF, Excel, reportes programados
- [ ] **Integraci√≥n con SIEM**: Conectores para sistemas de seguridad
- [ ] **An√°lisis predictivo**: Predicci√≥n de anomal√≠as futuras

### üóìÔ∏è Historial de Versiones

- **v1.0.0** (Actual)
  - Detecci√≥n de anomal√≠as con Isolation Forest
  - Explicaciones con modelos LLM configurables
  - Interfaz web completa con Vue 3
  - Soporte para archivos grandes
  - M√∫ltiples bases de datos integradas

## Mejores Pr√°cticas de Uso

### Para An√°lisis de Seguridad
1. **Configurar alertas**: Establecer umbrales para anomal√≠as cr√≠ticas
2. **Revisar regularmente**: Programar an√°lisis autom√°ticos
3. **Correlacionar eventos**: Analizar patrones en ventanas de tiempo
4. **Mantener contexto**: Incluir logs de m√∫ltiples fuentes

### Para Optimizaci√≥n de Rendimiento
1. **Ajustar chunk size**: Balancear memoria vs velocidad
2. **Usar GPU cuando est√© disponible**: Significativa mejora en LLM
3. **Limpiar regularmente**: Eliminar archivos temporales antiguos
4. **Monitorear recursos**: Verificar uso de CPU, RAM y almacenamiento

### Para Desarrollo y Testing
1. **Usar modelos ligeros**: `phi3:mini` para desarrollo r√°pido
2. **Configurar logs detallados**: Para debugging efectivo
3. **Probar con datos reales**: Validar con logs de producci√≥n
4. **Documentar configuraciones**: Mantener registro de par√°metros

## Agradecimientos

- **Ollama** por proporcionar una excelente plataforma para modelos LLM locales
- **FastAPI** por el framework web r√°pido y robusto
- **Vue.js** por el framework frontend intuitivo
- **Scikit-learn** por los algoritmos de machine learning
- Comunidad de c√≥digo abierto por las bibliotecas y herramientas

---

**üìù Nota**: Este README se actualiza regularmente. Para la informaci√≥n m√°s reciente, consulte la [documentaci√≥n completa](https://github.com/ctangarife/reto-ia-log-analyzer/wiki) o los [issues del proyecto](https://github.com/ctangarife/reto-ia-log-analyzer/issues).
