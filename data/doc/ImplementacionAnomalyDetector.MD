# ğŸš€ ImplementaciÃ³n del Servicio Anomaly Detector

## ğŸ“‹ Resumen del Proyecto

Se ha implementado exitosamente el servicio `anomaly-detector` para detectar anomalÃ­as en logs usando Isolation Forest y generar explicaciones con un LLM local (Ollama/Gemma).

## ğŸ—ï¸ Arquitectura Implementada

### Estructura de Directorios
```
logsanomaly/
â”œâ”€â”€ ğŸ”¨ build/
â”‚   â””â”€â”€ ğŸ“ anomaly-detector/
â”‚       â””â”€â”€ ğŸ“„ Dockerfile
â”œâ”€â”€ ğŸ“Š data/
â”‚   â”œâ”€â”€ ğŸ“ anomaly-detector/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ main.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ requirements.txt
â”‚   â”‚   â”œâ”€â”€ ğŸ“ config/
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ config.yml
â”‚   â”‚   â”œâ”€â”€ ğŸ“ scripts/
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ test_service.sh
â”‚   â”‚   â”œâ”€â”€ ğŸ“ tests/
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ test_anomaly_detection.py
â”‚   â”‚   â””â”€â”€ ğŸ“ reports/
â”‚   â””â”€â”€ ğŸ“ models/
â”‚       â””â”€â”€ ğŸ“ ollama/
â”œâ”€â”€ ğŸ³ docker-compose.yml
â””â”€â”€ ğŸ“ bd/
```

## ğŸ”§ Componentes Implementados

### 1. Servicio FastAPI (`main.py`)
- **Endpoints implementados:**
  - `GET /health` - Health check
  - `POST /detect` - DetecciÃ³n desde archivo
  - `POST /detect-text` - DetecciÃ³n desde JSON

### 2. DetecciÃ³n de AnomalÃ­as
- **Algoritmo:** Isolation Forest de scikit-learn
- **CaracterÃ­sticas extraÃ­das:**
  - Longitud del log
  - NÃºmero de palabras
  - EntropÃ­a del texto
  - Presencia de palabras clave sospechosas
  - NÃºmero de caracteres especiales
  - NÃºmero de nÃºmeros
  - Longitud promedio de palabras

### 3. IntegraciÃ³n con LLM
- **Servicio:** Ollama con modelo Nidum-Gemma-2B-Uncensored-GGUF
- **Prompt:** "Explica en lenguaje natural por quÃ© este log parece sospechoso: {log_entry}"
- **ConfiguraciÃ³n:** Temperatura 0.7, mÃ¡ximo 150 tokens
- **Soporte GPU:** NVIDIA GPU con 16GB de memoria reservada
- **CuantizaciÃ³n:** Q4_K_M (7.77 GB) para balance Ã³ptimo entre rendimiento y memoria

### 4. GeneraciÃ³n de Reportes
- **Formato:** JSON con timestamp
- **UbicaciÃ³n:** `/app/reports/report-{timestamp}.json`
- **Contenido:** Logs analizados, anomalÃ­as detectadas, explicaciones

## ğŸ³ ConfiguraciÃ³n Docker

### Dockerfile
- **Imagen base:** Python 3.11-slim
- **Puerto:** 8000
- **Dependencias:** FastAPI, scikit-learn, pandas, requests

### Docker Compose
- **Servicios:**
  - `anomaly-detector`: Servicio principal
  - `ollama-service`: LLM local con soporte GPU
- **Red:** `logs_anomaly_net` con aliases
- **VolÃºmenes:** CÃ³digo, reportes, modelos
- **GPU:** Reserva de 16GB de memoria y acceso a todas las GPUs NVIDIA

## ğŸ“Š Funcionalidades

### DetecciÃ³n de AnomalÃ­as
1. **Entrada:** Archivo de logs o JSON con logs
2. **Procesamiento:** ExtracciÃ³n de caracterÃ­sticas
3. **AnÃ¡lisis:** Isolation Forest para detectar outliers
4. **ExplicaciÃ³n:** LLM genera explicaciÃ³n para cada anomalÃ­a
5. **Salida:** JSON con resultados y reporte guardado

### Palabras Clave Sospechosas
- error, failed, unauthorized, exception
- timeout, denied, critical, fatal
- panic, abort

## ğŸ§ª Testing

### Scripts de Prueba
- **`test_service.sh`:** Pruebas de integraciÃ³n
- **`test_anomaly_detection.py`:** Pruebas unitarias con pytest

### Casos de Prueba
- Health check
- ExtracciÃ³n de caracterÃ­sticas
- DetecciÃ³n de anomalÃ­as
- Endpoints con archivo y JSON
- Manejo de errores

## ğŸš€ Uso del Servicio

### 1. Levantar los Servicios
```bash
docker-compose up -d
```

### 2. Verificar Health Check
```bash
curl http://localhost:8000/health
```

### 3. Detectar AnomalÃ­as desde Archivo
```bash
curl -X POST "http://localhost:8000/detect" \
     -H "Content-Type: multipart/form-data" \
     -F "file=@logs.txt"
```

### 4. Detectar AnomalÃ­as desde JSON
```bash
curl -X POST "http://localhost:8000/detect-text" \
     -H "Content-Type: application/json" \
     -d '[{"content": "2024-01-01 ERROR Failed to connect"}]'
```

## ğŸ“ˆ Ejemplo de Respuesta

```json
{
  "total_logs": 5,
  "anomalies_detected": 2,
  "anomalies": [
    {
      "log_entry": "2024-01-01 10:04:00 CRITICAL System memory usage exceeded 95%",
      "anomaly_score": -0.15,
      "is_anomaly": true,
      "explanation": "Este log es sospechoso porque indica un uso crÃ­tico de memoria del 95%, lo cual es una condiciÃ³n anÃ³mala que puede causar fallos del sistema."
    }
  ],
  "report_file": "report-20241109_094200.json"
}
```

## ğŸ”§ ConfiguraciÃ³n

### Variables de Entorno
- `OLLAMA_SERVICE_URL`: URL del servicio Ollama
- `MODEL_NAME`: Nombre del modelo LLM (nidum-gemma-2b-uncensored-gguf)
- `OLLAMA_DEVICE`: Dispositivo para inferencia (nvidia)
- `PYTHONUNBUFFERED`: Para logs en tiempo real

### Archivo de ConfiguraciÃ³n
- **UbicaciÃ³n:** `data/anomaly-detector/config/config.yml`
- **Contenido:** ParÃ¡metros de detecciÃ³n, LLM, palabras clave

## ğŸ“ PrÃ³ximos Pasos

1. **OptimizaciÃ³n:** Ajustar parÃ¡metros de Isolation Forest
2. **Modelos:** Probar otros modelos LLM
3. **UI:** Crear interfaz web para visualizaciÃ³n
4. **Alertas:** Implementar sistema de notificaciones
5. **MÃ©tricas:** Agregar monitoreo y mÃ©tricas

## ğŸ¯ Estado del Proyecto

âœ… **Completado:**
- Estructura de directorios
- Dockerfile y docker-compose
- Servicio FastAPI completo
- DetecciÃ³n de anomalÃ­as con Isolation Forest
- IntegraciÃ³n con LLM (Ollama/Gemma)
- GeneraciÃ³n de reportes
- Scripts de prueba
- DocumentaciÃ³n

ğŸ”„ **En Progreso:**
- Pruebas de integraciÃ³n
- OptimizaciÃ³n de parÃ¡metros

ğŸ“‹ **Pendiente:**
- Interfaz web
- Sistema de alertas
- Monitoreo avanzado

---

**Fecha de implementaciÃ³n:** 9 de noviembre de 2024  
**VersiÃ³n:** 1.0.0  
**Estado:** Funcional y listo para pruebas
