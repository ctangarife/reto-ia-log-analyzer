# Detector de AnomalÃ­as en Logs

Sistema de detecciÃ³n de anomalÃ­as en logs utilizando Isolation Forest y LLM (Large Language Model) para anÃ¡lisis y explicaciÃ³n en lenguaje natural.

## Ãndice
1. [DescripciÃ³n General](#descripciÃ³n-general)
2. [Arquitectura](#arquitectura)
3. [InstalaciÃ³n Paso a Paso](#instalaciÃ³n-paso-a-paso)
4. [ConfiguraciÃ³n](#configuraciÃ³n)
5. [Uso del Sistema](#uso-del-sistema)
6. [Modelos LLM Soportados](#modelos-llm-soportados)
7. [Estructura de Directorios](#estructura-de-directorios)
8. [Flujos Principales](#flujos-principales)
9. [CaracterÃ­sticas Clave](#caracterÃ­sticas-clave)
10. [Limitaciones y Consideraciones](#limitaciones-y-consideraciones)
11. [SoluciÃ³n de Problemas (FAQ)](#soluciÃ³n-de-problemas-faq)

## DescripciÃ³n General

El detector de anomalÃ­as en logs es un sistema completo que combina algoritmos de machine learning (Isolation Forest) con modelos de lenguaje (LLM) para:

1. Procesar archivos de logs de gran tamaÃ±o
2. Detectar patrones anÃ³malos o sospechosos
3. Proporcionar explicaciones en lenguaje natural sobre las anomalÃ­as detectadas
4. Visualizar resultados de manera intuitiva

### ðŸ† CaracterÃ­sticas Destacadas

- âœ… **Procesamiento de archivos grandes** (GB de logs)
- âœ… **Modelo de IA configurable** (cualquier modelo de Ollama)
- âœ… **Interfaz web intuitiva** con drag & drop
- âœ… **AnÃ¡lisis en tiempo real** con streaming de resultados
- âœ… **Explicaciones en lenguaje natural** de las anomalÃ­as
- âœ… **Historial persistente** de anÃ¡lisis
- âœ… **Escalable** con Docker y microservicios
- âœ… **Soporte GPU/CPU** para mejor rendimiento

### ðŸ“Š Stack TecnolÃ³gico

| Componente | TecnologÃ­a | VersiÃ³n | PropÃ³sito |
|-----------|------------|--------|----------|
| Frontend | Vue 3 + Vite | 3.3.0 | Interfaz de usuario |
| UI Library | PrimeVue | 3.40.0 | Componentes UI |
| Backend | FastAPI + Uvicorn | 0.104.1 | API REST |
| ML Engine | Scikit-learn | 1.3.2 | Isolation Forest |
| LLM Service | Ollama | 0.5.8 | Modelos de lenguaje |
| Databases | MongoDB + PostgreSQL + Redis | 7.0 / 15 / 7.2 | Almacenamiento |
| Proxy | Nginx | stable-alpine | Reverse proxy |
| Containerization | Docker + Compose | - | OrquestaciÃ³n |

## Arquitectura

El sistema estÃ¡ compuesto por los siguientes servicios en contenedores Docker:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Vue UI     â”‚     â”‚  FastAPI Server â”‚     â”‚  Ollama Service â”‚
â”‚   (Frontend)    â”‚â”€â”€â”€â”€â–¶â”‚(Anomaly Detect) â”‚â”€â”€â”€â”€â–¶â”‚    (LLM)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–²                       â”‚                        â”‚
        â”‚                       â–¼                        â”‚
        â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚    Nginx        â”‚              â–¼
                      â”‚  (Proxy Server)  â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  Base de Datos   â”‚
                                             â”‚  (MongoDB, etc.) â”‚
                                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Componentes

1. **Frontend (Vue3)**
   - Interfaz web para subida de archivos y visualizaciÃ³n de resultados
   - Manejo de archivos grandes mediante chunking
   - VisualizaciÃ³n en tiempo real del procesamiento
   - Historial de anÃ¡lisis persistente

2. **Backend (FastAPI)**
   - API REST para procesamiento de logs
   - DetecciÃ³n de anomalÃ­as usando Isolation Forest
   - IntegraciÃ³n con Ollama para explicaciones en lenguaje natural
   - Procesamiento por chunks y streaming de resultados

3. **LLM (Ollama)**
   - Servicio local de LLM (modelo configurable)
   - Por defecto usa Qwen 2.5 3B, pero se puede usar cualquier modelo compatible con Ollama
   - GeneraciÃ³n de explicaciones en lenguaje natural
   - Procesamiento por lotes para optimizaciÃ³n

4. **Nginx**
   - Proxy inverso
   - Manejo de archivos grandes
   - ConfiguraciÃ³n para streaming

5. **Bases de Datos**
   - MongoDB: Almacenamiento de logs y reportes
   - PostgreSQL: GestiÃ³n de usuarios y configuraciones
   - Redis: CachÃ© y gestiÃ³n de colas de procesamiento

## Estructura de Directorios

```
reto-ia-log-analyzer/
â”œâ”€â”€ build/                      # Dockerfiles y configuraciones
â”‚   â”œâ”€â”€ anomaly-detector/      # Servicio de detecciÃ³n
â”‚   â”‚   â””â”€â”€ Dockerfile         # Imagen del backend
â”‚   â”œâ”€â”€ ollama/               # Servicio LLM
â”‚   â”‚   â”œâ”€â”€ Dockerfile         # Imagen de Ollama
â”‚   â”‚   â””â”€â”€ init-ollama.sh     # Script de inicializaciÃ³n
â”‚   â”œâ”€â”€ ui/                   # Frontend
â”‚   â”‚   â””â”€â”€ Dockerfile         # Imagen del frontend
â”‚   â”œâ”€â”€ mongodb/              # ConfiguraciÃ³n MongoDB
â”‚   â”‚   â””â”€â”€ init-mongo.js      # Script de inicializaciÃ³n
â”‚   â””â”€â”€ redis/                # ConfiguraciÃ³n Redis
â”‚       â””â”€â”€ redis.conf         # Archivo de configuraciÃ³n
â”œâ”€â”€ data/                       # CÃ³digo de la aplicaciÃ³n
â”‚   â”œâ”€â”€ anomaly-detector/      # Backend (FastAPI)
â”‚   â”‚   â”œâ”€â”€ main.py            # API principal
â”‚   â”‚   â”œâ”€â”€ requirements.txt    # Dependencias Python
â”‚   â”‚   â”œâ”€â”€ config/            # Configuraciones
â”‚   â”‚   â”œâ”€â”€ database/          # Scripts de BD
â”‚   â”‚   â”‚   â””â”€â”€ init.sql       # InicializaciÃ³n PostgreSQL
â”‚   â”‚   â”œâ”€â”€ scripts/           # Scripts auxiliares
â”‚   â”‚   â”œâ”€â”€ chunks/            # Almacenamiento temporal
â”‚   â”‚   â””â”€â”€ reports/           # Reportes generados
â”‚   â”œâ”€â”€ ui/                    # Frontend (Vue3)
â”‚   â”‚   â”œâ”€â”€ package.json       # Dependencias Node.js
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ components/    # Componentes Vue
â”‚   â”‚   â”‚   â”œâ”€â”€ stores/        # Estado global (Pinia)
â”‚   â”‚   â”‚   â””â”€â”€ utils/         # Utilidades
â”‚   â”‚   â””â”€â”€ dist/              # Archivos compilados
â”‚   â”œâ”€â”€ models/                # Modelos LLM descargados
â”‚   â”‚   â””â”€â”€ ollama/            # Modelos de Ollama
â”‚   â””â”€â”€ static/                # Archivos estÃ¡ticos servidos por Nginx
â”œâ”€â”€ nginx/                      # ConfiguraciÃ³n del proxy
â”‚   â””â”€â”€ nginx.conf             # ConfiguraciÃ³n Nginx
â””â”€â”€ docker-compose.yml          # OrquestaciÃ³n de servicios
```

## Flujos Principales

### 1. Procesamiento de Logs

1. **Subida y Chunking**
   ```javascript
   // Frontend: Divide archivo en chunks manejables
   const chunks = await splitLogFile(file)  // 500KB por chunk
   for (const chunk of chunks) {
     const formData = new FormData()
     formData.append('file', createChunkFile(chunk))
     // EnvÃ­o y procesamiento streaming...
   }
   ```

2. **DetecciÃ³n de AnomalÃ­as**
   ```python
   # Backend: Procesa cada chunk
   def detect_anomalies(log_lines):
       features = extract_features(log_lines)
       scores = isolation_forest.predict(features)
       return process_anomalies(log_lines, scores)
   ```

3. **ExplicaciÃ³n LLM**
   ```python
   # Backend: Procesa anomalÃ­as en lotes
   async def process_anomalies_batch(anomalies):
       tasks = [get_llm_explanation(a) for a in anomalies]
       explanations = await asyncio.gather(*tasks)
       return combine_results(anomalies, explanations)
   ```

### 2. GestiÃ³n de Estado

1. **Store Global (Pinia)**
   ```typescript
   // Frontend: Manejo de estado
   const analysisStore = defineStore('analysis', {
     state: () => ({
       analysisHistory: [],
       currentAnalysis: null
     }),
     actions: {
       addAnalysis(result) {
         // ActualizaciÃ³n de histÃ³rico...
       }
     }
   })
   ```

2. **Persistencia de Resultados**
   ```python
   # Backend: Guarda resultados por archivo
   def save_report(file_id, results):
       report_path = f"/app/chunks/{file_id}/report_{timestamp}.json"
       with open(report_path, 'w') as f:
           json.dump(results, f)
   ```

## CaracterÃ­sticas Clave

1. **Procesamiento de Archivos Grandes**
   - DivisiÃ³n en chunks de 500KB
   - Procesamiento incremental
   - Streaming de resultados
   - Progreso en tiempo real

2. **DetecciÃ³n de AnomalÃ­as**
   - Uso de Isolation Forest
   - Features: longitud, entropÃ­a, palabras clave
   - Scoring y clasificaciÃ³n
   - Procesamiento paralelo

3. **Explicaciones IA**
   - Modelo local Nidum-Gemma-2B
   - Procesamiento por lotes
   - Prompts optimizados
   - Respuestas estructuradas

4. **UI/UX**
   - Carga de archivos con drag & drop
   - VisualizaciÃ³n en tiempo real
   - Historial persistente
   - AgrupaciÃ³n por archivos

## Ejemplos de Uso y Resultados

### Ejemplo de Log Normal
```
2024-01-15 10:30:15 INFO [UserService] User login successful: user123@example.com
2024-01-15 10:30:16 INFO [OrderService] Order created: ID=12345, User=user123
2024-01-15 10:30:17 INFO [PaymentService] Payment processed: $25.99
```
**Resultado**: Sin anomalÃ­as detectadas

### Ejemplo de Log AnÃ³malo
```
2024-01-15 10:30:15 ERROR [AuthService] Multiple failed login attempts from 192.168.1.100
2024-01-15 10:30:16 ERROR [AuthService] SQL injection attempt detected: admin' OR '1'='1
2024-01-15 10:30:17 CRITICAL [SecurityService] Unauthorized access attempt to /admin/users
```
**Resultado**: âš ï¸ **AnomalÃ­a detectada** - Score: -0.85  
**ExplicaciÃ³n IA**: "Se detecta un posible ataque de fuerza bruta seguido de intento de inyecciÃ³n SQL y acceso no autorizado. Recomiendo bloquear la IP 192.168.1.100 y revisar los logs de seguridad."

### Demo en Vivo

Puede probar el sistema con archivos de ejemplo:
1. `logs/ejemplo_normal.log` - Logs tÃ­picos de aplicaciÃ³n
2. `logs/ejemplo_anomalias.log` - Logs con patrones sospechosos
3. `logs/ejemplo_mixto.log` - CombinaciÃ³n de logs normales y anÃ³malos

## InstalaciÃ³n Paso a Paso

> ðŸš€ **Â¿Tienes prisa?** Ve a la [GuÃ­a de InstalaciÃ³n RÃ¡pida](./INSTALACION-RAPIDA.md) para tenerlo funcionando en 5 minutos.

### 1. Requisitos Previos

- **Sistema Operativo**: Windows, macOS o Linux
- **Docker**: v20.10 o superior
- **Docker Compose**: v2.0 o superior
- **Hardware Recomendado**:
  - CPU: 4 cores o mÃ¡s
  - RAM: 16GB mÃ­nimo (32GB recomendado para mejores resultados)
  - Almacenamiento: 10GB libres mÃ­nimo
  - GPU: NVIDIA compatible con CUDA (opcional, mejora significativamente el rendimiento del LLM)

### 2. InstalaciÃ³n BÃ¡sica

```bash
# Clonar el repositorio
git clone https://github.com/ctangarife/reto-ia-log-analyzer.git
cd reto-ia-log-analyzer

# Levantar los servicios
docker-compose up -d
```

### 3. VerificaciÃ³n de InstalaciÃ³n

Una vez iniciados los servicios, puede verificar que todo estÃ© funcionando correctamente:

```bash
# Ver estado de los contenedores
docker-compose ps

# Ver logs de los servicios
docker-compose logs -f
```

Acceda a la interfaz web a travÃ©s de http://localhost:80

## ConfiguraciÃ³n

### ConfiguraciÃ³n del LLM (Modelo de Lenguaje)

**El modelo de lenguaje es totalmente configurable y opcional**. Por defecto, el sistema usa `qwen2.5:3b`, pero puede configurar cualquier modelo compatible con Ollama.

Para cambiar el modelo, edite las siguientes lÃ­neas en el archivo `docker-compose.yml`:

```yaml
# En el servicio anomaly-detector
environment:
  - OLLAMA_SERVICE_URL=http://ollama-service:11434
  - MODEL_NAME=qwen2.5:3b  # Cambie a su modelo preferido

# En el servicio ollama-service
environment:
  - OLLAMA_HOST=0.0.0.0
  - OLLAMA_DEVICE=nvidia
  - OLLAMA_MODEL=qwen2.5:3b  # Cambie a su modelo preferido
```

Modelos recomendados:
- `llama3:8b` - Mayor calidad pero requiere mÃ¡s recursos
- `gemma:7b` - Buen balance calidad/rendimiento
- `phi3:mini` - Modelo ligero para equipos con recursos limitados
- Cualquier modelo compatible con Ollama

### ConfiguraciÃ³n del Sistema

#### Ajustes de Hardware

Ajuste los lÃ­mites de recursos en `docker-compose.yml` segÃºn las capacidades de su sistema:

```yaml
# Para el servicio de Ollama (LLM)
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: all
          capabilities: [ gpu ]
    limits:
      memory: 16G  # Ajuste segÃºn la RAM disponible
```

#### ParÃ¡metros de DetecciÃ³n

Para ajustar la sensibilidad del detector de anomalÃ­as, edite el archivo `data/anomaly-detector/main.py`:

```python
isolation_forest = IsolationForest(
    contamination=0.1,  # Porcentaje de anomalÃ­as esperado (0.1 = 10%)
    random_state=42,
    n_estimators=100  # MÃ¡s estimadores = mayor precisiÃ³n pero mÃ¡s lento
)
```

## Uso del Sistema

### 1. Acceso a la Interfaz

Abra su navegador y vaya a http://localhost:80

### 2. AnÃ¡lisis de Logs

1. **Subir Archivo**: Arrastre y suelte su archivo de logs o haga clic en el Ã¡rea de carga
2. **ConfiguraciÃ³n de AnÃ¡lisis**: Establezca los parÃ¡metros segÃºn necesite
3. **Iniciar AnÃ¡lisis**: Haga clic en el botÃ³n "Analizar"
4. **Ver Resultados**: Los resultados se mostrarÃ¡n en tiempo real a medida que se procesan

### 3. GestiÃ³n de Reportes

- Los reportes se guardan automÃ¡ticamente y estÃ¡n disponibles en la secciÃ³n "Historial"
- Cada reporte incluye estadÃ­sticas generales y detalles de las anomalÃ­as detectadas
- Puede exportar los resultados en formato JSON o CSV

## Modelos LLM Soportados

El sistema es compatible con cualquier modelo disponible en Ollama. La elecciÃ³n del modelo dependerÃ¡ del hardware disponible y la calidad deseada de las explicaciones.

### Â¿CÃ³mo cambiar el modelo?

1. **OpciÃ³n 1**: Cambiar la variable de entorno en docker-compose.yml (como se explicÃ³ anteriormente)

2. **OpciÃ³n 2**: Usar un modelo ya descargado
   ```bash
   # Primero descargar el modelo deseado
   docker exec logs-analyze-ollama ollama pull llama3:8b
   
   # Luego editar docker-compose.yml y reiniciar los servicios
   docker-compose down
   docker-compose up -d
   ```

3. **OpciÃ³n 3**: Crear un modelo personalizado
   ```bash
   # Conectarse al contenedor de Ollama
   docker exec -it logs-analyze-ollama bash
   
   # Crear un modelo personalizado
   cat > /tmp/Modelfile << EOF
   FROM llama3:8b
   PARAMETER temperature 0.7
   PARAMETER stop "User:"
   PARAMETER stop "Assistant:"
   EOF
   
   # Registrar el modelo
   ollama create mi-modelo-personalizado -f /tmp/Modelfile
   
   # Salir del contenedor
   exit
   ```
   Luego actualice las variables de entorno en docker-compose.yml con `MODEL_NAME=mi-modelo-personalizado`

### ConfiguraciÃ³n Avanzada de Modelos

Para configuraciones mÃ¡s detalladas, ejemplos especÃ­ficos por hardware y scripts de automatizaciÃ³n, consulte el documento [CONFIGURACION-MODELOS.md](./CONFIGURACION-MODELOS.md).

## Limitaciones y Consideraciones

1. **Rendimiento**
   - TamaÃ±o de chunk afecta memoria y velocidad
   - LLM puede ser cuello de botella (especialmente sin GPU)
   - Considerar batch size vs latencia
   - Tiempo de descarga inicial del modelo puede ser significativo

2. **Almacenamiento**
   - Chunks y reportes ocupan espacio
   - Modelos LLM pueden requerir varios GB de almacenamiento
   - Implementar limpieza periÃ³dica
   - Monitorear uso de disco

3. **Escalabilidad**
   - Vertical: Aumentar recursos (especialmente RAM para modelos grandes)
   - Horizontal: MÃºltiples workers
   - CachÃ© de LLM para respuestas comunes
   - Considerar modelos mÃ¡s pequeÃ±os para mayor eficiencia

## SoluciÃ³n de Problemas (FAQ)

### Problemas Comunes de InstalaciÃ³n

#### Error: "No se puede conectar a Docker"
**SoluciÃ³n**:
1. Verificar que Docker Desktop estÃ© ejecutÃ¡ndose
2. Verificar permisos: `docker run hello-world`
3. Reiniciar Docker Desktop si es necesario

#### Error: "Out of memory" durante la descarga del modelo
**SoluciÃ³n**:
```yaml
# En docker-compose.yml, reducir lÃ­mites de memoria:
deploy:
  resources:
    limits:
      memory: 8G  # Reducir de 16G a 8G
```
O usar un modelo mÃ¡s pequeÃ±o como `phi3:mini`

#### El servicio Ollama no responde
**SoluciÃ³n**:
```bash
# Verificar estado del contenedor
docker logs logs-analyze-ollama

# Si necesita reiniciar solo el servicio Ollama
docker-compose restart ollama-service
```

### Problemas de Rendimiento

#### El anÃ¡lisis de logs es muy lento
**Soluciones**:
1. **Sin GPU**: Usar modelos mÃ¡s pequeÃ±os
   ```yaml
   environment:
     - MODEL_NAME=phi3:mini  # Modelo ligero
   ```

2. **Con GPU**: Verificar que NVIDIA Docker estÃ© instalado
   ```bash
   # Verificar GPU disponible
   docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi
   ```

3. **Ajustar tamaÃ±o de chunk** en el backend

#### La descarga del modelo toma demasiado tiempo
**Opciones**:
1. Usar un modelo predesacargado
2. Cambiar a un modelo mÃ¡s pequeÃ±o temporalmente
3. Verificar conexiÃ³n a internet

### Problemas de ConfiguraciÃ³n

#### Â¿CÃ³mo usar el sistema sin GPU?
**ConfiguraciÃ³n**:
```yaml
# Comentar o eliminar la secciÃ³n de GPU en docker-compose.yml
# deploy:
#   resources:
#     reservations:
#       devices:
#         - driver: nvidia
#           count: all
#           capabilities: [ gpu ]

# Cambiar variables de entorno
environment:
  - OLLAMA_DEVICE=cpu
  - MODEL_NAME=phi3:mini  # Usar modelo ligero
```

#### Â¿CÃ³mo cambiar la base de datos?
Por defecto usa MongoDB, PostgreSQL y Redis. Para usar solo una:
```yaml
# En docker-compose.yml, comentar los servicios no deseados
# y ajustar las variables de entorno en anomaly-detector
```

#### Â¿CÃ³mo ajustar la sensibilidad de detecciÃ³n?
Edite `data/anomaly-detector/main.py`:
```python
# Valores mÃ¡s bajos = mÃ¡s sensible (mÃ¡s anomalÃ­as detectadas)
isolation_forest = IsolationForest(
    contamination=0.05,  # Cambiar de 0.1 a 0.05 para mÃ¡s sensibilidad
    random_state=42,
    n_estimators=100
)
```

### Comandos Ãštiles

#### Monitoreo del Sistema
```bash
# Ver uso de recursos
docker stats

# Ver logs en tiempo real
docker-compose logs -f

# Ver logs de un servicio especÃ­fico
docker-compose logs -f anomaly-detector

# Ver modelos descargados
docker exec logs-analyze-ollama ollama list
```

#### Limpieza y Mantenimiento
```bash
# Limpiar archivos temporales
docker-compose down
docker system prune -f

# Limpiar volÃºmenes (Â¡CUIDADO: Elimina todos los datos!)
docker-compose down -v

# Reconstruir imÃ¡genes
docker-compose build --no-cache
```

#### Backup y RestauraciÃ³n
```bash
# Backup de los datos
docker run --rm -v logsanomaly_mongodb_data:/data -v $(pwd):/backup ubuntu tar czf /backup/mongodb-backup.tar.gz -C /data .

# Backup de los modelos
docker run --rm -v logs-analyze-ollama_models:/models -v $(pwd):/backup ubuntu tar czf /backup/models-backup.tar.gz -C /models .
```

### Contacto y Soporte

- **Repositorio**: [GitHub](https://github.com/ctangarife/reto-ia-log-analyzer)
- **Issues**: Para reportar bugs o solicitar caracterÃ­sticas en [GitHub Issues](https://github.com/ctangarife/reto-ia-log-analyzer/issues)
- **DocumentaciÃ³n**: Wiki del repositorio para informaciÃ³n adicional

### Contribuir al Proyecto

1. Fork del repositorio
2. Crear rama para nueva caracterÃ­stica: `git checkout -b feature/nueva-caracteristica`
3. Commit de cambios: `git commit -am 'Add nueva-caracteristica'`
4. Push a la rama: `git push origin feature/nueva-caracteristica`
5. Crear Pull Request

## Licencia

Este proyecto estÃ¡ licenciado bajo la Licencia MIT. Consulte el archivo [LICENSE](LICENSE) para mÃ¡s detalles.

## Roadmap y CaracterÃ­sticas Futuras

### ðŸ—ºï¸ PrÃ³ximas CaracterÃ­sticas

- [ ] **API de integraciÃ³n**: Endpoints para sistemas externos
- [ ] **Alertas en tiempo real**: Notificaciones por email/Slack
- [ ] **Dashboard avanzado**: MÃ©tricas y grÃ¡ficos detallados
- [ ] **Modelos personalizados**: Entrenamiento con datos especÃ­ficos
- [ ] **Soporte multi-tenant**: SeparaciÃ³n por organizaciones
- [ ] **ExportaciÃ³n avanzada**: PDF, Excel, reportes programados
- [ ] **IntegraciÃ³n con SIEM**: Conectores para sistemas de seguridad
- [ ] **AnÃ¡lisis predictivo**: PredicciÃ³n de anomalÃ­as futuras

### ðŸ—“ï¸ Historial de Versiones

- **v1.0.0** (Actual)
  - DetecciÃ³n de anomalÃ­as con Isolation Forest
  - Explicaciones con modelos LLM configurables
  - Interfaz web completa con Vue 3
  - Soporte para archivos grandes
  - MÃºltiples bases de datos integradas

## Mejores PrÃ¡cticas de Uso

### Para AnÃ¡lisis de Seguridad
1. **Configurar alertas**: Establecer umbrales para anomalÃ­as crÃ­ticas
2. **Revisar regularmente**: Programar anÃ¡lisis automÃ¡ticos
3. **Correlacionar eventos**: Analizar patrones en ventanas de tiempo
4. **Mantener contexto**: Incluir logs de mÃºltiples fuentes

### Para OptimizaciÃ³n de Rendimiento
1. **Ajustar chunk size**: Balancear memoria vs velocidad
2. **Usar GPU cuando estÃ© disponible**: Significativa mejora en LLM
3. **Limpiar regularmente**: Eliminar archivos temporales antiguos
4. **Monitorear recursos**: Verificar uso de CPU, RAM y almacenamiento

### Para Desarrollo y Testing
1. **Usar modelos ligeros**: `phi3:mini` para desarrollo rÃ¡pido
2. **Configurar logs detallados**: Para debugging efectivo
3. **Probar con datos reales**: Validar con logs de producciÃ³n
4. **Documentar configuraciones**: Mantener registro de parÃ¡metros

## Agradecimientos

- **Ollama** por proporcionar una excelente plataforma para modelos LLM locales
- **FastAPI** por el framework web rÃ¡pido y robusto
- **Vue.js** por el framework frontend intuitivo
- **Scikit-learn** por los algoritmos de machine learning
- Comunidad de cÃ³digo abierto por las bibliotecas y herramientas

---

**ðŸ“ Nota**: Este README se actualiza regularmente. Para la informaciÃ³n mÃ¡s reciente, consulte la [documentaciÃ³n completa](https://github.com/ctangarife/reto-ia-log-analyzer/wiki) o los [issues del proyecto](https://github.com/ctangarife/reto-ia-log-analyzer/issues).
